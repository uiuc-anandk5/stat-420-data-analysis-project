---
title: "Modeling Udemy Course Popularity"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(
  scipen = 1,
  digits = 4,
  width = 80,
  fig.align = "center"
)
```


## Introduction

**Introduction to the data**

In this project, we are working with data about different courses on the online learning platform Udemy (found [here on Kaggle](https://www.kaggle.com/datasets/andrewmvd/udemy-courses)). The dataset has 3682 records of courses, each with 12 different variables, and was created by a data scientist in Brazil on May 16, 2020. The most important are:
- Course title
- Whether the course is free or paid (true or false)
- Price of course (in Brazilian Real)
- Number of subscribers
- Number of reviews
- Number of lectures
- Level/difficulty of course (all, beginner, intermediate, expert)
- Course duration (in hours)
- Date the course was published
- Course subject (web development, business finance, musical instruments, graphic design)
Some pairs of variables are closely related, but we take that into account when building our models and completing the analysis.

**Goal of our model**

Our goal is to develop a model that can predict the number of subscribers of a course, based on other information about that course. In practice, this model could be used to predict which courses are most likely to grow more popular, giving the platform insight into which courses they may want to promote.
 
**Why we chose this dataset**

All of our team members are working / have worked in education-related careers, so we really wanted to work on an education-based dataset. Amanda is currently an online course developer for MathWorks on Coursera, so sheâ€™s particularly interested in MOOC platform data analysis. Dani does research and analysis at a higher education institution that is planning a large expansion of online offerings.  Anand is a solutions architect at a not-for-profit and has worked on several projects to improve postsecondary education for students by enabling researchers to generate rigorous evidence and help those in the field connect research and practice. 


## Methods

### Importing the Data and Feature Descriptions

First, we downloaded the original data file from Kaggle, as linked in the introduction. 

```{r}
library(readr)
udemy = read_csv("udemy_courses.csv", show_col_types = FALSE)
```

We then examined the features to determine which were relevant to us. The feature descriptions and our determination on their relevance are below:

Response: num_subscribers
Predictors:
- course_id: randomized id assigned to each course (not relevant)
- course_title: title of the course (possibly relevant with some text processing, but we didn't have time to pursue this option)
- url: url of the course (not relevant)
- is_paid: boolean representing if the course is paid (relevant)
- price: price of the course in Brazilian Real as of 5/16/20 (relevant)
- num_reviews: number of reviews for the course as of 5/16/20 (relevant)
- num_lectures: number of lectures in the course as of 5/16/20 (relevant)
- level: level of difficulty of the course-- either all, beginner, intermediate or expert (relevant)
- content_duration: duration of the course in hours (relevant)
- published_timestamp: date the course was published, represented as a string (relevant after processing)
- subject: subject the course falls within, either web development, business finance, musical instruments, or graphic design (relevant)

### Data Preprocessing

Next, we preprocessed the data by removing irrelevant features, and coercing "level", "subject", and "is_paid" to factor variables. We also used the "published_timestamp" information to instead create a new column called "days", which represents the number of days from the course's release to the day the data was retrieved. After this new column was created, we deleted "published_timestamp". The other column we altered was the "price" column, which we converted from Brazilian Real to USD according to the exchange rate on the day of data retrieval. Finally, we removed the only course in the data that did not have any lectures or content, since that shouldn't even be a valid course on Udemy.

```{r}
# Removing irrelevant features
udemy = subset(udemy,select=-c(course_id,url,course_title))

# Coercing level and subject to factor variables
udemy$level = as.factor(udemy$level)
udemy$subject = as.factor(udemy$subject)
udemy$is_paid = as.factor(udemy$is_paid)

# Replacing timestamp with days since the course was released
dates = as.Date(substr(udemy$published_timestamp,1,10))
data_retrieval_date = as.Date('2020-05-16')
udemy$days = as.integer(data_retrieval_date - dates)
udemy = subset(udemy,select=-c(published_timestamp))

# Replacing price in BRL with price in USD according to
#  the exchange rate of 0.1708 on 5/16/2020
udemy$price = udemy$price * 0.1708

# Removing course #893 which had no lectures or content duration
udemy = udemy[c(1:892,894:3677),]
```

### Initial Data Exploration

After preprocessing the data, we wanted to explore the data a little bit more, so we looked at a summary of the dataset and plotted a few graphs that helped us better understand the relationships between our response variable, "num_subscribers", and some of the other quantitative variables. While not directly related to our model exploration, it did help us decide which features we more strongly wanted to perform a log transform on. The code and results for this section can be found in [Appendix Part A](#Part A). ____TO ADD: include why we did the "pairs" and "cor" code too! the code itself is already included in the appendix with the rest. Also a discussion on why we looked at skew_______

### Model Exploration

When trying to identify an ideal model, we experimented with many different combinations of predictors and used transforms, interactions, and polynomial models to try and produce the best result. In the end, we came up with the following 5 combinations of response & predictors (labeled in the code as "scope1", "scope2", etc.). Some of these used log transforms of the response, and others did not.

```{r}
scope1 = log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) + log(content_duration) * log(num_lectures) + log(days) + subject + level
scope2 = num_subscribers ~ is_paid + I(price^2) + num_reviews + I(num_lectures^2) + level + I(days^2) + subject
scope3 = log1p(num_subscribers) ~ is_paid + log1p(price) + log1p(num_reviews) + log(num_lectures) + level + subject + log(days) + log(num_lectures):log(content_duration)
scope4 = log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) + log(num_lectures) + log(days) + subject
scope5 = num_subscribers ~ num_reviews * is_paid + price + level + days + subject
```

Afterwards, we divided the data into a training set and testing set, using a 70-30 split. We only used the training data to train our models, and the testing data to evaluate and compare our models.

```{r}
udemy_train_idx = sample(nrow(udemy), size = trunc(0.70 * nrow(udemy)))
udemy_train = udemy[udemy_train_idx,]
udemy_test = udemy[-udemy_train_idx,]
```

At this point, we wanted to see if it would be beneficial to only train the models on the non-influential observations in the training set, while evaluating them on all the observations (both influential and not) later on with the testing set. We ended up training each of the 5 models in 2 ways-- once on all the training data, and once on only the non-influential points of the training data. The models trained on all the training data are referred to as "model1_all", "model2_all", etc and the models trained on only the non-influential points are called "model1_without_influential", "model2_without_influential", etc. Here, we identify influential points using the metric from class, namely all points where the Cook's Distance is greater than $4/n$, $n$ being the number of observations in the data set.

```{r}
library(caret)
train.control <- trainControl(method = "cv", number = 10)

model1_all = train(scope1, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist = cooks.distance(model1_all$finalModel)
model1_without_influential = train(scope1, data = udemy_train, subset = cooks_dist <= 4 / length(cooks_dist), method = "lm", trControl = train.control)

model2_all = train(scope2, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist = cooks.distance(model2_all$finalModel)
model2_without_influential = train(scope2, data = udemy_train, subset = cooks_dist <= 4 / length(cooks_dist), method = "lm", trControl = train.control)

model3_all = train(scope3, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist = cooks.distance(model3_all$finalModel)
model3_without_influential = train(scope3, data = udemy_train, subset = cooks_dist <= 4 / length(cooks_dist), method = "lm", trControl = train.control)

model4_all = train(scope4, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist = cooks.distance(model4_all$finalModel)
model4_without_influential = train(scope4, data = udemy_train, subset = cooks_dist <= 4 / length(cooks_dist), method = "lm", trControl = train.control)

model5_all = train(scope5, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist = cooks.distance(model5_all$finalModel)
model5_without_influential = train(scope5, data = udemy_train, subset = cooks_dist <= 4 / length(cooks_dist), method = "lm", trControl = train.control)
```


```{r}
model_names = c('Model 1', 'Model 1 w/o outliers', 'Model 2', 'Model 2 w/o outliers', 'Model 3', 'Model 3 w/o outliers', 'Model 4', 'Model 4 w/o outliers', 'Model 5', 'Model 5 w/o outliers')
adj_r_2 = c(summary(model1_all)$adj.r.squared, summary(model1_without_influential)$adj.r.squared,
            summary(model2_all)$adj.r.squared, summary(model2_without_influential)$adj.r.squared,
            summary(model3_all)$adj.r.squared, summary(model3_without_influential)$adj.r.squared,
            summary(model4_all)$adj.r.squared, summary(model4_without_influential)$adj.r.squared,
            summary(model5_all)$adj.r.squared, summary(model5_without_influential)$adj.r.squared)

get_rmse = function(model, log){
  preds = if(log == TRUE) {expm1(predict(model, newdata = udemy_test))}
          else {predict(model, newdata = udemy_test)}
  sqrt(mean((preds - udemy_test$num_subscribers) ^ 2))
}

test_rmses = c(get_rmse(model1_all, TRUE), get_rmse(model1_without_influential, TRUE),
               get_rmse(model2_all, FALSE), get_rmse(model2_without_influential, FALSE),
               get_rmse(model3_all, TRUE), get_rmse(model3_without_influential, TRUE),
               get_rmse(model4_all, TRUE), get_rmse(model4_without_influential, TRUE),
               get_rmse(model5_all, FALSE), get_rmse(model5_without_influential, FALSE)) 
  
get_mape = function(model, log){
  preds = if(log == TRUE) {expm1(predict(model, newdata = udemy_test))}
          else {predict(model, newdata = udemy_test)}
  sum(abs(udemy_test$num_subscribers - preds)) / sum(udemy_test$num_subscribers)
}

test_mapes = c(get_mape(model1_all, TRUE), get_mape(model1_without_influential, TRUE),
               get_mape(model2_all, FALSE), get_mape(model2_without_influential, FALSE),
               get_mape(model3_all, TRUE), get_mape(model3_without_influential, TRUE),
               get_mape(model4_all, TRUE), get_mape(model4_without_influential, TRUE),
               get_mape(model5_all, FALSE), get_mape(model5_without_influential, FALSE))

get_aic = function(model, log){
  resid = if(log == TRUE) {expm1(fitted(model)) - udemy_train$num_subscribers}
            else {resid(model)}
  n = length(resid(model))
  p = length(coef(model))
  n * log(mean(resid ^2)) + 2 * p
}

test_aic = c(get_aic(model1_all$finalModel, TRUE), get_aic(model1_without_influential$finalModel, TRUE),
               get_aic(model2_all$finalModel, FALSE), get_aic(model2_without_influential$finalModel, FALSE),
               get_aic(model3_all$finalModel, TRUE), get_aic(model3_without_influential$finalModel, TRUE),
               get_aic(model4_all$finalModel, TRUE), get_aic(model4_without_influential$finalModel, TRUE),
               get_aic(model5_all$finalModel, FALSE), get_aic(model5_without_influential$finalModel, FALSE))

get_bic = function(model, log){
  resid = if(log == TRUE) {expm1(fitted(model)) - udemy_train$num_subscribers}
            else {resid(model)}
  n = length(resid(model))
  p = length(coef(model))
  n * log(mean(resid ^2)) + log(n) * p
}

test_bic = c(get_bic(model1_all$finalModel, TRUE), get_bic(model1_without_influential$finalModel, TRUE),
               get_bic(model2_all$finalModel, FALSE), get_bic(model2_without_influential$finalModel, FALSE),
               get_bic(model3_all$finalModel, TRUE), get_bic(model3_without_influential$finalModel, TRUE),
               get_bic(model4_all$finalModel, TRUE), get_bic(model4_without_influential$finalModel, TRUE),
               get_bic(model5_all$finalModel, FALSE), get_bic(model5_without_influential$finalModel, FALSE))

num_params = c(length(coef(model1_all$finalModel)),length(coef(model1_without_influential$finalModel)),
              length(coef(model2_all$finalModel)),length(coef(model2_without_influential$finalModel)),
              length(coef(model3_all$finalModel)),length(coef(model3_without_influential$finalModel)),
              length(coef(model4_all$finalModel)),length(coef(model4_without_influential$finalModel)),
              length(coef(model5_all$finalModel)),length(coef(model5_without_influential$finalModel)))

criterion = data.frame(
  'Criterion' = model_names,
  'Adjusted R^2' = adj_r_2,
  'RMSE' = test_rmses,
  'WMAPE' = test_mapes,
  'AIC' = test_aic,
  'BIC' = test_bic,
  'Num Params' = num_params
)
library(knitr)
kable(criterion)
```



```{r warning=FALSE}
library(leaps)
library(lmtest)

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (
    1 - hatvalues(model)
  )) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}

n = nrow(udemy)

udemy_model_dredge = lm(
  num_subscribers ~ is_paid + price + num_reviews +
    level + days + subject,
  data = udemy,
  na.action = na.fail
)

library(MuMIn)
combinations = dredge(
  udemy_model_dredge,
  extra = list(
    bptest = function(x)
      if (length(x$coefficients) > 1) {
        bptest(x)$p.value
      },
    
    loocv_rmse = function(x)
      get_loocv_rmse(x),
    
    adj_r2 = function(x)
      get_adj_r2(x)
    
  )
)


#Anand note - Select first result matching criteria (after sorting by bptest and filtering on
#loocv_rmse, adj_r2, and df in combinations dataframe)
#This is not necessarily the best model - just using a placeholder - need to try various transformations
selected_model = lm(
  num_subscribers ~ is_paid + price + num_reviews +
    level + days + subject,
  data = udemy
)

get_loocv_rmse(selected_model)
get_adj_r2(selected_model)
get_bp_decision(selected_model, alpha = 0.01)
get_num_params(selected_model)

```




*** TBD: Higher order terms

*** TBD: There appear to be outliers - investigate further and remove.

```{r}
library(lmtest)
bptest(selected_model)

shapiro.test(resid(selected_model))

plot(fitted(selected_model), resid(selected_model), col = "dodgerblue",
     pch = 20, cex = 1.5, xlab = "Fitted", ylab = "Residuals")
abline(h = 0, lty = 2, col = "darkorange", lwd = 2)

#Points of large leverage
sum(hatvalues(selected_model) > 2 * mean(hatvalues(selected_model)))

```

For the Breusch-Pagan test, if we see a small p-value, we reject the null of homoscedasticity. The constant variance assumption is violated.

For the Shapiro-Wilk test, the null hypothesis assumes the data were sampled from a normal distribution, thus a small p-value indicates we believe there is only a small probability the data could have been sampled from a normal distribution.



## Results

***
*** Delete before submitting

The results section should contain numerical or graphical summaries of your results. 

You should report a final model you have chosen.  

There is not necessarily one, singular correct model, but certainly some methods and models are better than others in certain situations. You may use any methods we studied this semester to complete this task, and provide evidence that your final choice of model is a good one.  

Some possible items to be discussed: 

***


## Discussion

***
*** Delete before submitting

The discussion section should contain discussion of your results and should frame your results in the context of the data.  
- How is your final model useful? 

***

## Appendix

### Appendix Part A {#PartA}

```{r, warning = FALSE}
library(dplyr)
glimpse(udemy)
summary(udemy)

library(faraway)
pairs(udemy, col = "dodgerblue")

# Correlation Matrix
library(ggcorrplot)
model.matrix(~0+., data=udemy) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag = F, type="lower", lab=TRUE, lab_size=2)

# Checking which variables have a lot of skew from normal 
library(moments)
skewness(udemy$days)
skewness(udemy$content_duration)
skewness(udemy$price)
skewness(udemy$num_lectures)

# Plots relating different predictors to the response
plot(x = udemy$price, y = log(udemy$num_subscribers),
     main = "Log Number of Subscribers vs. Price", 
     xlab = "Price (in USD)", ylab = "Log Number of Subscribers")

plot(x = log(udemy$num_reviews), y = log(udemy$num_subscribers),
     main = "Log Number of Subscribers vs. Log Number of Reviews", 
     xlab = "Log Number of Reviews", ylab = "Log Number of Subscribers")

plot(x = log(udemy$num_lectures), y = log(udemy$num_subscribers),
     main = "Log Number of Subscribers vs. Log Number of Lectures", 
     xlab = "Log Number of Lectures", ylab = "Log Number of Subscribers")

plot(x = log(udemy$content_duration), y = log(udemy$num_subscribers),
     main = "Log Number of Subscribers vs. Log Content Duration", 
     xlab = "Log Content Duration (in Hours)", ylab = "Log Number of Subscribers")

plot(x = udemy$days, y = log(udemy$num_subscribers),
     main = "Log Number of Subscribers vs. Days Since Release", 
     xlab = "Number of Days Since Course Release", ylab = "Log Number of Subscribers")
```

Group Members: Amanda Wang, Anand Kumar, Dani Richmond

