---
title: "Modeling Udemy Course Popularity"
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
urlcolor: cyan
---

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(
  scipen = 1,
  digits = 4,
  width = 80,
  fig.align = "center"
)
```


## Introduction

**Introduction to the data**

In this project, we are working with data about different courses on the online learning platform Udemy (found [here on Kaggle](https://www.kaggle.com/datasets/andrewmvd/udemy-courses)). The dataset has 3,682 records of courses, each with 12 different variables, and was created by a data scientist in Brazil on May 16, 2020. The most important are:

- Course title
- Whether the course is free or paid (true or false)
- Price (in Brazilian Real)
- Number of subscribers
- Number of reviews
- Number of lectures
- Level/difficulty of course (all, beginner, intermediate, expert)
- Course duration (in hours)
- Date the course was published
- Course subject (web development, business finance, musical instruments, graphic design)

Some pairs of variables are closely related, but we take that into account when building our models and completing the analysis.

**Goal of our model**

Our goal is to develop a model that can predict the number of subscribers for a course, based on other information about that course. In practice, this model could be used to predict which courses are most likely to grow more popular, giving the Udemy platform insight into which courses they may want to promote.
 
**Why we chose this dataset**

All of our team members are working / have worked in education-related careers, so we really wanted to work on an education-based dataset. Amanda is currently an online course developer for MathWorks on Coursera, so sheâ€™s particularly interested in MOOC platform data analysis. Dani does research and analysis at a higher education institution that is planning a large expansion of online offerings.  Anand is a solutions architect at a not-for-profit and has worked on several projects to improve postsecondary education for students by enabling researchers to generate rigorous evidence and help those in the field connect research and practice. 


## Methods

### Importing the Data and Feature Descriptions

First, we downloaded the original data file from Kaggle, as linked in the introduction. 

```{r warning=FALSE}
library(readr)
udemy = read_csv("udemy_courses.csv", show_col_types = FALSE)
```

We then examined the features to determine which were relevant to us. The feature descriptions and our determination on their relevance are below:

Response: num_subscribers

Predictors:

- course_id: randomized id assigned to each course (not relevant)
- course_title: title of the course (possibly relevant with some text processing, but we didn't have time to pursue this option)
- url: url of the course (not relevant)
- is_paid: boolean representing if the course is paid (relevant)
- price: price of the course in Brazilian Real as of 5/16/20 (relevant)
- num_reviews: number of reviews for the course as of 5/16/20 (relevant)
- num_lectures: number of lectures in the course as of 5/16/20 (relevant)
- level: level of difficulty of the course-- either all, beginner, intermediate or expert (relevant)
- content_duration: duration of the course in hours (relevant)
- published_timestamp: date the course was published, represented as a string (relevant after processing)
- subject: subject the course falls within, either web development, business finance, musical instruments, or graphic design (relevant)

### Data Preprocessing

Next, we preprocessed the data by removing irrelevant features, and coercing "level", "subject", and "is_paid" to factor variables. We also used the "published_timestamp" information to instead create a new column called "days", which represents the number of days from the course's release to the day the data was retrieved. After this new column was created, we removed "published_timestamp". The other column we altered was the "price" column, which we converted from Brazilian Real to USD according to the exchange rate on the day of data retrieval. Finally, we removed the only course in the data that did not have any lectures or content, since that shouldn't even be a valid course on Udemy.

```{r}
# Removing irrelevant features
udemy = subset(udemy, select = -c(course_id, url, course_title))

# Coercing level and subject to factor variables
udemy$level = as.factor(udemy$level)
udemy$subject = as.factor(udemy$subject)
udemy$is_paid = as.factor(udemy$is_paid)

# Replacing timestamp with days since the course was released
dates = as.Date(substr(udemy$published_timestamp, 1, 10))
data_retrieval_date = as.Date('2020-05-16')
udemy$days = as.integer(data_retrieval_date - dates)
udemy = subset(udemy, select = -c(published_timestamp))

# Replacing price in BRL with price in USD according to
#  the exchange rate of 0.1708 on 5/16/2020
udemy$price = udemy$price * 0.1708

# Removing course #893 which had no lectures or content duration
udemy = udemy[c(1:892, 894:3677), ]
```

### Initial Data Exploration

After preprocessing the data, we wanted to explore the data a little bit more, so we looked at a summary of the dataset and plotted a few graphs that helped us better understand the relationships between our response variable, `num_subscribers`, and some of the other quantitative variables. While not directly related to our model exploration, these plots helped us decide which features we wanted to perform a log transformation on. We also used the skewness() function from the Moments library to determine which variables had more skew from the normal distribution and thus would be good candidates for transformations. This showed that both `content_duration` and `num_lectures` were skewed which aligns with what we found when skimming the data - a subset of courses with very large numbers of hours and lectures. Additionally, we used pairs plots and a correlation matrix to see which variables may have collinearity issues. We discovered that `content_duration` and `num_lectures` were positively correlated with each other which makes sense intuitively. 
The code and results for this section can be found in [Appendix Part A](#Part A). 

### Model Exploration

When trying to identify an ideal model, we experimented with many different combinations of predictors and used transformations, interaction terms, polynomial terms, and models from forward/backward AIC/BIC step selection to try to produce the best result. We did not include this experimental code here or in the appendix for brevity sake. In the end, we came up with the following 5 combinations of response & predictors (labeled in the code as "scope1", "scope2", etc.). Some of these used log transformations of the response and others did not.

```{r}
scope1 = log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) +
  log(content_duration) * log(num_lectures) + log(days) + subject + level

scope2 = num_subscribers ~ is_paid + I(price ^ 2) + num_reviews + I(num_lectures ^ 2) +
  level + I(days ^ 2) + subject

scope3 = log1p(num_subscribers) ~ is_paid + log1p(price) + log1p(num_reviews) +
  log(num_lectures) + level + subject + log(days) + log(num_lectures):log(content_duration)

scope4 = log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) + log(num_lectures) +
  log(days) + subject

scope5 = num_subscribers ~ num_reviews * is_paid + price + level + days + subject
```

Afterwards, we divided the data into a training set and testing set, using a 70-30 split. We only used the training data to train our models, and the testing data to evaluate and compare our models.

```{r}
udemy_train_idx = sample(nrow(udemy), size = trunc(0.70 * nrow(udemy)))
udemy_train = udemy[udemy_train_idx, ]
udemy_test = udemy[-udemy_train_idx, ]
```

At this point, we wanted to see if it would be beneficial to only train the models on the non-influential observations in the training set, while evaluating them on all the observations (both influential and not) later on with the testing set. We ended up training each of the 5 models in 2 ways-- once on all the training data, and once on only the non-influential points of the training data. The models trained on all the training data are referred to as "model1_all", "model2_all", etc and the models trained on only the non-influential points are called "model1_without_influential", "model2_without_influential", etc. Here, we identify influential points using the metric from class, namely all points where the Cook's Distance is greater than $4/n$, $n$ being the number of observations in the data set. More specifically, we used the `train` function from the caret library in order to fine-tune the parameters for each model by doing 10-fold cross-validation. This function outputs a final model for each model.

```{r warning=FALSE, message=FALSE}
library(caret)
train.control <- trainControl(method = "cv", number = 10)

model1_all = train(scope1, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist1 = cooks.distance(model1_all$finalModel)
noninfl_idx1 = cooks_dist1 <= 4 / length(cooks_dist1)
model1_without_influential = train(scope1, data = udemy_train, subset = noninfl_idx1, method = "lm", 
                trControl = train.control)

model2_all = train(scope2, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist2 = cooks.distance(model2_all$finalModel)
noninfl_idx2 = cooks_dist2 <= 4 / length(cooks_dist2)
model2_without_influential = train(scope2, data = udemy_train, subset = noninfl_idx2, method = "lm",
                  trControl = train.control)

model3_all = train(scope3, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist3 = cooks.distance(model3_all$finalModel)
noninfl_idx3 = cooks_dist3 <= 4 / length(cooks_dist3)
model3_without_influential = train(scope3, data = udemy_train, subset = noninfl_idx3, method = "lm",
                  trControl = train.control)

model4_all = train(scope4, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist4 = cooks.distance(model4_all$finalModel)
noninfl_idx4 = cooks_dist4 <= 4 / length(cooks_dist4)
model4_without_influential = train(scope4, data = udemy_train, subset = noninfl_idx4, method = "lm",
                  trControl = train.control)

model5_all = train(scope5, data = udemy_train, method = "lm", trControl = train.control)
cooks_dist5 = cooks.distance(model5_all$finalModel)
noninfl_idx5 = cooks_dist5 <= 4 / length(cooks_dist5)
model5_without_influential = train(scope5, data = udemy_train, subset = noninfl_idx5, method = "lm",
                trControl = train.control)
```

Now that we had the models trained, we wanted to compare them using a variety of different metrics, including  adjusted $R^2$, RMSE, WAPE, AIC, and BIC. Because RMSE is biased towards larger models, we also added a column for the number of parameters in the model so that we could compare that as well. The following code shows the calculation of these metrics, and displays the result in an easy-to-read table. Since some of our models include a log transformation of the response variable while others did not, we had to write functions for most of the metrics to derive comparisons on the same scale across models. More details about each section of the code can be found in the comments.

```{r}
# Vector representing names of the models
model_names = c(
  'Model 1',
  'Model 1 w/o outliers',
  'Model 2',
  'Model 2 w/o outliers',
  'Model 3',
  'Model 3 w/o outliers',
  'Model 4',
  'Model 4 w/o outliers',
  'Model 5',
  'Model 5 w/o outliers'
)

# List of all models
models = list(model1_all,
              model1_without_influential,
              model2_all,
              model2_without_influential,
              model3_all,
              model3_without_influential,
              model4_all,
              model4_without_influential,
              model5_all,
              model5_without_influential)

# Function to get adjusted r squared value
get_r2 = function(model){
  summary(model)$adj.r.squared
}

# Vector representing the adjusted r squared values for each model
adj_r_2 = as.double(lapply(models, get_r2))

# Function to get the RMSE value from a model, depending on if the response is log transformed
get_rmse = function(model, log) {
  preds = if (log == TRUE) {
    expm1(predict(model, newdata = udemy_test))
  }
  else {
    predict(model, newdata = udemy_test)
  }
  sqrt(mean((preds - udemy_test$num_subscribers) ^ 2))
}

# Vector of the RMSE values
test_rmses = c(
  get_rmse(model1_all, TRUE),
  get_rmse(model1_without_influential, TRUE),
  get_rmse(model2_all, FALSE),
  get_rmse(model2_without_influential, FALSE),
  get_rmse(model3_all, TRUE),
  get_rmse(model3_without_influential, TRUE),
  get_rmse(model4_all, TRUE),
  get_rmse(model4_without_influential, TRUE),
  get_rmse(model5_all, FALSE),
  get_rmse(model5_without_influential, FALSE)
)

# Function to calculate the weighted MAPE depending on if the response is log transformed
get_wape = function(model, log) {
  preds = if (log == TRUE) {
    expm1(predict(model, newdata = udemy_test))
  }
  else {
    predict(model, newdata = udemy_test)
  }
  (sum(abs(udemy_test$num_subscribers - preds)) / sum(udemy_test$num_subscribers)) * 100
}

# Vector of WAPE values
test_wapes = c(
  get_wape(model1_all, TRUE),
  get_wape(model1_without_influential, TRUE),
  get_wape(model2_all, FALSE),
  get_wape(model2_without_influential, FALSE),
  get_wape(model3_all, TRUE),
  get_wape(model3_without_influential, TRUE),
  get_wape(model4_all, TRUE),
  get_wape(model4_without_influential, TRUE),
  get_wape(model5_all, FALSE),
  get_wape(model5_without_influential, FALSE)
)

# Vector of indices of non-influential points (to be used in the calculations of AIC and BIC)
noninfl_idx = cbind(noninfl_idx1,
                    noninfl_idx2,
                    noninfl_idx3,
                    noninfl_idx4,
                    noninfl_idx5)

# Function to manually calculate AIC because of log transformed responses
get_aic = function(model, log, infl, model_num) {
  resid = if (log == TRUE &
              infl == TRUE) {
    expm1(fitted(model$finalModel)) - udemy_train$num_subscribers
  }
  else if (log == TRUE) {
    expm1(fitted(model$finalModel)) - udemy_train$num_subscribers[noninfl_idx[, model_num]]
  }
  else {
    resid(model$finalModel)
  }
  n = length(resid(model$finalModel))
  p = length(coef(model$finalModel))
  n * log(mean(resid ^ 2)) + 2 * p
}

# Vector of AIC values
test_aic = c(
  get_aic(model1_all, TRUE, TRUE, 1),
  get_aic(model1_without_influential, TRUE, FALSE, 1),
  get_aic(model2_all, FALSE, TRUE, 2),
  get_aic(model2_without_influential, FALSE, FALSE, 2),
  get_aic(model3_all, TRUE, TRUE, 3),
  get_aic(model3_without_influential, TRUE, FALSE, 3),
  get_aic(model4_all, TRUE, TRUE, 4),
  get_aic(model4_without_influential, TRUE, FALSE, 4),
  get_aic(model5_all, FALSE, TRUE, 5),
  get_aic(model5_without_influential, FALSE, FALSE, 5)
)

# Function to manually calculate BIC because of the log transformed responses
get_bic = function(model, log, infl, model_num) {
  resid = if (log == TRUE &
              infl == TRUE) {
    expm1(fitted(model$finalModel)) - udemy_train$num_subscribers
  }
  else if (log == TRUE) {
    expm1(fitted(model$finalModel)) - udemy_train$num_subscribers[noninfl_idx[, model_num]]
  }
  else {
    resid(model$finalModel)
  }
  n = length(resid(model$finalModel))
  p = length(coef(model$finalModel))
  n * log(mean(resid ^ 2)) + log(n) * p
}

# Vector of BIC values
test_bic = c(
  get_bic(model1_all, TRUE, TRUE, 1),
  get_bic(model1_without_influential, TRUE, FALSE, 1),
  get_bic(model2_all, FALSE, TRUE, 2),
  get_bic(model2_without_influential, FALSE, FALSE, 2),
  get_bic(model3_all, TRUE, TRUE, 3),
  get_bic(model3_without_influential, TRUE, FALSE, 3),
  get_bic(model4_all, TRUE, TRUE, 4),
  get_bic(model4_without_influential, TRUE, FALSE, 4),
  get_bic(model5_all, FALSE, TRUE, 5),
  get_bic(model5_without_influential, FALSE, FALSE, 5)
)

# Function to get number of parameters in a model
get_params = function(model){
  length(coef(model$finalModel))
}

# Vector representing the number of parameters in each model
num_params = as.double(lapply(models, get_params))
```

### Checking Assumptions

Before settling on a final model, we wanted to check how the models did with regards to the constant variance assumption (with the Breusch-Pagan test) and the normality assumption (with the Shapiro-Wilk test).

```{r warning=FALSE, message=FALSE}
# Function to get BP Test p-value
get_bp = function(model){
  signif(bptest(model$finalModel)$p.value, 2)
}

# Vector representing the BP Test p-values of each model
library(lmtest)
bptest = as.character(as.double(lapply(models, get_bp)))

# Function to get Shapiro Test p-values
get_shapiro = function(model, log, infl, model_num) {
  resid = if (log == TRUE &
              infl == TRUE) {
    expm1(fitted(model$finalModel)) - udemy_train$num_subscribers
  }
  else if (log == TRUE) {
    expm1(fitted(model$finalModel)) - udemy_train$num_subscribers[noninfl_idx[, model_num]]
  }
  else {
    resid(model$finalModel)
  }
  signif(shapiro.test(resid)$p.value, 2)
}

# Vector containing the Shapiro test results
shapiro = c(
  as.character(get_shapiro(model1_all, TRUE, TRUE, 1)),
  get_shapiro(model1_without_influential, TRUE, FALSE, 1),
  get_shapiro(model2_all, FALSE, TRUE, 2),
  get_shapiro(model2_without_influential, FALSE, FALSE, 2),
  get_shapiro(model3_all, TRUE, TRUE, 3),
  get_shapiro(model3_without_influential, TRUE, FALSE, 3),
  get_shapiro(model4_all, TRUE, TRUE, 4),
  get_shapiro(model4_without_influential, TRUE, FALSE, 4),
  get_shapiro(model5_all, FALSE, TRUE, 5),
  get_shapiro(model5_without_influential, FALSE, FALSE, 5)
)


# Putting all the data together in a table
criterion = data.frame(
  'Criterion' = model_names,
  'Adjusted R^2' = adj_r_2,
  'RMSE' = test_rmses,
  'WAPE' = test_wapes,
  'AIC' = test_aic,
  'BIC' = test_bic,
  'Num Params' = num_params,
  'BP Test' = bptest,
  'Shapiro Test' = shapiro
)

```

## Results

```{r}
library(knitr)
kable(criterion)
```

In selecting a final model we saught a balance of the following metrics:

- higher adjusted $R^2$: indicates more of the variance is explained by the model than by noise
- lower RMSE: indicates less error between the predicted and actual values although in a biased manner
- lower WAPE: the Weighted Absolute Error Percentage is a more unbiased metric for indicating less error between the predicted and actual values; it also works better for scenarios like ours where the actual values range from very small to quite large since traditional MAPE over-emphasizes differences for small values.
- lower AIC: indicates a balance between errors and number of predictors used
- lower BIC: also indicates a balance between errors and number of predictors used but penalizes larger models more
- lower Number of Parameters: indicates the model is less complex which makes it easier to understand and interpret results
- higher BP Test: indicates constant variance assumption is not violated
- higher SW Test: indicates normality assumption is not violated

Unfortunately none of our models produced a p-value that would pass either the Breusch-Pagan test or the Shapiro-Wilkes test. In our initial model exploration we specifically tested every model (50+) on these two tests and were never able to find a model that had a sufficiently high enough p-value. We decided to instead focus more on the other metrics represented in the table above.

For most of the metrics, our models trained without outliers performed better on the test dataset than those that included outliers. 

***
*** Delete before submitting

The results section should contain numerical or graphical summaries of your results. 

You should report a final model you have chosen.  

There is not necessarily one, singular correct model, but certainly some methods and models are better than others in certain situations. You may use any methods we studied this semester to complete this task, and provide evidence that your final choice of model is a good one.  

Some possible items to be discussed: 

***

```{r}
calc_weights = function(min_or_max,values,weight) {
  
  if(min_or_max == "min") {
    extrema =  ( 1 / ( values / min(values) ) ) * weight
  } else {
    extrema = ( values / max(values) ) * weight
  }
  
  extrema
}

weighted_adj_r_2 = calc_weights("max", adj_r_2, 0.25) # 25%
weighted_test_rmses = calc_weights("min", test_rmses, 0.0) # 0%
weighted_test_wapes = calc_weights("min", test_wapes, 0.25) # 25%
weighted_test_aic = calc_weights("min", test_aic, (0.50/3) ) # 16.67%
weighted_test_bic = calc_weights("min", test_bic, (0.50/3) ) # 16.67%
weighted_num_params = calc_weights("min", num_params, (0.50/3) ) # 16.67%

weighted_sum = weighted_adj_r_2 + weighted_test_rmses + weighted_test_wapes + 
  weighted_test_aic  + weighted_test_bic  + weighted_num_params

# Putting all the data together in a table
criterion = data.frame(
  'Model' = model_names,
  'W Adj R^2' = weighted_adj_r_2,
  'W RMSE' = weighted_test_rmses,
  'W WAPE' = weighted_test_wapes,
  'W AIC' = weighted_test_aic,
  'W BIC' = weighted_test_bic,
  'W Num Params' = weighted_num_params,
  'Weighted Total' = weighted_sum
)
library(knitr)
kable(criterion)
```




## Discussion

***
*** Delete before submitting

The discussion section should contain discussion of your results and should frame your results in the context of the data.  
- How is your final model useful? 

***

## Appendix

### Appendix Part A {#PartA}

```{r, warning = FALSE, message=FALSE}
library(dplyr)
glimpse(udemy)
summary(udemy)

library(faraway)
pairs(udemy, col = "dodgerblue")

# Correlation Matrix
library(ggcorrplot)
model.matrix( ~ 0 + ., data = udemy) %>%
  cor(use = "pairwise.complete.obs") %>%
  ggcorrplot(
    show.diag = F,
    type = "lower",
    lab = TRUE,
    lab_size = 2
  )

# Checking which variables have a lot of skew from normal
library(moments)
skewness(udemy$days)
skewness(udemy$content_duration)
skewness(udemy$price)
skewness(udemy$num_lectures)

# Plots relating different predictors to the response
plot(
  x = udemy$price,
  y = log(udemy$num_subscribers),
  main = "Log Number of Subscribers vs. Price",
  xlab = "Price (in USD)",
  ylab = "Log Number of Subscribers"
)

plot(
  x = log(udemy$num_reviews),
  y = log(udemy$num_subscribers),
  main = "Log Number of Subscribers vs. Log Number of Reviews",
  xlab = "Log Number of Reviews",
  ylab = "Log Number of Subscribers"
)

plot(
  x = log(udemy$num_lectures),
  y = log(udemy$num_subscribers),
  main = "Log Number of Subscribers vs. Log Number of Lectures",
  xlab = "Log Number of Lectures",
  ylab = "Log Number of Subscribers"
)

plot(
  x = log(udemy$content_duration),
  y = log(udemy$num_subscribers),
  main = "Log Number of Subscribers vs. Log Content Duration",
  xlab = "Log Content Duration (in Hours)",
  ylab = "Log Number of Subscribers"
)

plot(
  x = udemy$days,
  y = log(udemy$num_subscribers),
  main = "Log Number of Subscribers vs. Days Since Release",
  xlab = "Number of Days Since Course Release",
  ylab = "Log Number of Subscribers"
)
```

Group Members: Amanda Wang, Anand Kumar, Dani Richmond

