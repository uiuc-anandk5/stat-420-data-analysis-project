
```{r}
# checking metrics
library(caret)
train.control <- trainControl(method = "cv", number = 10)
scope1 = log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) + log(content_duration) * log(num_lectures) + log(days) + subject + level
model1 = train(scope1, data = udemy, method = "lm", trControl = train.control)
scope2 = num_subscribers ~ is_paid + I(price^2) + num_reviews + I(num_lectures^2) + level + I(days^2) + subject
model2 = train(scope2, data = udemy, method = "lm", trControl = train.control)
scope3 = log1p(num_subscribers) ~  is_paid + log1p(price) + log1p(num_reviews) + log(num_lectures) + level  + subject + log(days) + log(num_lectures):log(content_duration)
model3 = train(scope3, data = udemy, method = "lm", trControl = train.control)
scope4 = log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) + log(num_lectures) + log(days) + subject
model4 = train(scope4, data = udemy, method = "lm", trControl = train.control)
scope5 = num_subscribers ~ num_reviews * is_paid + price + level + days + subject
model5 = train(scope5, data = udemy, method = "lm", trControl = train.control)
criterion = data.frame(
  'Criterion' = c('Model 1', 'Model 2', 'Model 3','Model 4', 'Model 5'),
  'Adjusted R^2' = c(summary(model1)$adj.r.squared, 
                     summary(model2)$adj.r.squared,
                     summary(model3)$adj.r.squared,
                     summary(model4)$adj.r.squared,
                     summary(model5)$adj.r.squared),
  'RMSE' = c(expm1(model1$results$RMSE),
             model2$results$RMSE,
             expm1(model3$results$RMSE),
             expm1(model4$results$RMSE),
             model5$results$RMSE),
  'BP Test' = c(as.character(bptest(model1$finalModel)$p.value),
                bptest(model2$finalModel)$p.value,
                bptest(model3$finalModel)$p.value,
                bptest(model4$finalModel)$p.value,
                bptest(model5$finalModel)$p.value),
  'Shapiro Test' = c(as.character(shapiro.test(resid(model1$finalModel))$p.value),
                     shapiro.test(resid(model2$finalModel))$p.value,
                     shapiro.test(resid(model3$finalModel))$p.value,
                     shapiro.test(resid(model4$finalModel))$p.value,
                     shapiro.test(resid(model5$finalModel))$p.value),
  'Num Params' = c(length(coef(model1$finalModel)),
                   length(coef(model2$finalModel)),
                   length(coef(model3$finalModel)),
                   length(coef(model4$finalModel)),
                   length(coef(model5$finalModel)))
)
library(knitr)
kable(criterion)
```

We notice potential outliers.  Let's find large residuals.


```{r}

#Large residuals
( large_residuals = rstandard(model4$finalModel)[abs(rstandard(model4$finalModel)) > 2] )
length(large_residuals)

```


Let's find influential observations

```{r}

#Influential observations
cooks_distance = cooks.distance(model4$finalModel)
sum(cooks.distance(model4$finalModel) > 4 / length(cooks.distance(model4$finalModel)))

large_influences = cooks.distance(model4$finalModel) > 
  4 / length(cooks.distance(model4$finalModel))
cooks_distance[large_influences]


```

What happens if we remove these observations?

```{r}
#Anand note - This is not necessarily the best model - just using a placeholder -
# Modify this when modifying earlier chunk
selected_model_fixed = lm(
  log1p(num_subscribers) ~ log1p(price) + log1p(num_reviews) + log(num_lectures) + log(days) + subject, data = udemy,
  subset = cooks_distance <= 4 / length(cooks_distance)
)
coef(selected_model_fixed)

#Original model
par(mfrow = c(2, 2))
plot(model4$finalModel)

#Model after removing influential observations
par(mfrow = c(2, 2))
plot(selected_model_fixed)
```


```{r}
get_loocv_rmse(selected_model_fixed)
get_adj_r2(selected_model_fixed)
bptest(selected_model_fixed)$p.value
get_bp_decision(selected_model_fixed, alpha = 0.01)
shapiro.test(resid(selected_model_fixed))$p.value
get_sw_decision(selected_model_fixed, alpha = 0.01)
get_num_params(selected_model_fixed)

("original model4$finalModel")
get_loocv_rmse(model4$finalModel)
get_adj_r2(model4$finalModel)
bptest(model4$finalModel)$p.value
get_bp_decision(model4$finalModel, alpha = 0.01)
shapiro.test(resid(model4$finalModel))$p.value
get_sw_decision(model4$finalModel, alpha = 0.01)
get_num_params(model4$finalModel)

```


```{r}

#Influential observations
cooks_distance = cooks.distance(model3$finalModel)
sum(cooks.distance(model3$finalModel) > 4 / length(cooks.distance(model3$finalModel)))

large_influences = cooks.distance(model3$finalModel) > 
  4 / length(cooks.distance(model3$finalModel))
cooks_distance[large_influences]


```


```{r}
#Anand note - This is not necessarily the best model - just using a placeholder -
# Modify this when modifying earlier chunk
selected_model_fixed = lm(
  log1p(num_subscribers) ~  is_paid + log1p(price) + log1p(num_reviews) + log(num_lectures) + level  + subject + log(days) + log(num_lectures):log(content_duration), data = udemy,
  subset = cooks_distance <= 4 / length(cooks_distance)
)
coef(selected_model_fixed)

#Original model
par(mfrow = c(2, 2))
plot(model3$finalModel)

#Model after removing influential observations
par(mfrow = c(2, 2))
plot(selected_model_fixed)
```


```{r}
get_loocv_rmse(selected_model_fixed)
get_adj_r2(selected_model_fixed)
bptest(selected_model_fixed)$p.value
get_bp_decision(selected_model_fixed, alpha = 0.01)
shapiro.test(resid(selected_model_fixed))$p.value
get_sw_decision(selected_model_fixed, alpha = 0.01)
get_num_params(selected_model_fixed)

("original model3$finalModel")
get_loocv_rmse(model3$finalModel)
get_adj_r2(model3$finalModel)
bptest(model3$finalModel)$p.value
get_bp_decision(model3$finalModel, alpha = 0.01)
shapiro.test(resid(model3$finalModel))$p.value
get_sw_decision(model3$finalModel, alpha = 0.01)
get_num_params(model3$finalModel)

```
